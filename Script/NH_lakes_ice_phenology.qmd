---
title: "NH_lakes_ice_phenology"
author: "Aman Basu"
format: html
editor: visual
---

# Northern Hemisphere Lake Ice phenology

the Following packages are required for running the codes:

```{r}
#| message: false
#| warning: false

library(here) # path managment
library(tidyverse) # data curatioin and visualization
library(data.table) # fast reading and writing of data files

library(trend) # Mann-Kendall test and Sen's slope
library(mblm) # Sen's slope

library(blockCV) # For creating Spatial blocks
library(sf) # Spatial points 

library(doFuture) # parrallel processing
library(mlr3verse) # Machine learning package
library(mlr3spatiotempcv) # Spatiotemportal cross validation
library(ranger) # library for Random forest 

library(ggpubr) # for plotting
library(strucchange) # for breakpoint analysis
library(Metrics) # MAE,RMSE,SMAPE
library(randomForestExplainer) #random forest variable impotance 
```

### Reading Files

The files required for analysis is shared in the following link below;

\<link will be published after acceptance of the paper\>

```{r}
#| message: false
#| warning: false

# Lake freeze thaw Data with Meteorological data
NH_lakes_ts <- fread(here::here("Data",
                                "NH_plusCLIC_lakes_ts_version1.1.csv")) %>% 
  select(LAKEID,lakename,year,season,country,latitude,longitude,froze,
         ice_on,ice_on_doy,ice_off,ice_off_doy,PPT06:Tave05)

# Lake morphological data
hydrolakes <- fread(here::here("Data",
                               "NH_plusCLIC_lakes_hydrolakes_version1.1.csv")) %>% 
  select(LAKEID,lat,lon,Lake_area,Shore_len,Shore_dev,
         Vol_total,Depth_avg,Elevation,Wshd_area,Dmax_use_m)

# Co-ordinates of 1.24 million lakes with HydroLAKE IDs (Hylak_ID)
NH_coordinates <- read_csv(here::here("Data","NH_coordinates.csv"))

# morphological features of 1.24 million lakes with HydroLAKE IDs (Hylak_ID)
morph <- read_csv(here::here("Data","hydrolakes_morpho.csv"))

```

### Data curation for historical trend {data-link="Data curation for historical trend"}

```{r}
#Ice on historical trend data
NH_lake_ice_on_historical <- NH_lakes_ts %>% select(LAKEID,year,froze,ice_on_doy)
#Ice off historical trend data
NH_lake_ice_off_historical <- NH_lakes_ts %>% select(LAKEID,year,froze,ice_off_doy)
```

Ice duration calculation:

1.  duration is calculated by `Ice_off_doy - ice_on_doy`
2.  If lake did not freeze (froze column = "N") duration is imputed with `0`

```{r}

ice_duration_hist <- full_join(NH_lake_ice_on_historical,NH_lake_ice_off_historical, 
                          by = c("LAKEID" = "LAKEID", "year" = "year", "froze" = "froze")) %>% 
  mutate(duration = ice_off_doy - ice_on_doy) %>% na.omit() %>% 
  mutate(duration = ifelse(froze %in% "N", 0, duration)) %>% 
  select(LAKEID,year,ice_on_doy,ice_off_doy,duration)
```

### Ice records Imputation method {data-link="Ice records Imputation method"}

Imputation logic:

1.  When lake froze for an year or freezing status is unknown (froze column = "Y" or "U") average freezing date for that the lake over the TS is used.
2.  If lake did not freeze (froze column = "N"), latest TS freezing date for the lake is used.
3.  Above steps (i.e., 1 and 2) are carried out only if the lake time series has 20 or more years of data

Ice-on data:

```{r}
#| warning: false
# reqired ice-on data
req_ice_on <- NH_lakes_ts %>% select(LAKEID,year,froze,ice_on_doy,PPT06:Tave05)
# implimenting the two impututaion method the two exclusion critera 
ice_on_ts_imputed <- req_ice_on %>% group_by(LAKEID) %>%
  mutate(earliest_thawing_date = min(ice_on_doy,na.rm = T),
         average_ice_on = mean(ice_on_doy, na.rm = T)) %>% 
  filter_all(all_vars(!is.infinite(.))) %>% 
  mutate(ice_on_doy_imputed = 
           round(if_else((froze %in% "Y" | froze %in% "U") & 
                           ice_on_doy %in% NA & sum(!is.na(ice_on_doy)) >= 20,
                         average_ice_on,if_else((froze %in% "N" & length(LAKEID) >= 20),
                                                 earliest_thawing_date,
                                                 ice_on_doy))))%>% 
  select(LAKEID,year,ice_on_doy_imputed,PPT06:Tave05) 

#joining the hydrolakes data
ice_on_met_morph <- left_join(ice_on_ts_imputed,hydrolakes, by = "LAKEID") %>% 
  na.omit() %>% ungroup()
```

Ice-off data:

```{r}
#| warning: false
# required ice-off data
req_ice_off <- NH_lakes_ts %>% select(LAKEID,year,froze,ice_off_doy,PPT06:Tave05)
# implimenting the two impututaion method the two exclusion critera 
ice_off_ts_imputed <- req_ice_off %>% group_by(LAKEID) %>%
  mutate(earliest_thawing_date = min(ice_off_doy,na.rm = T),
         average_ice_off = mean(ice_off_doy, na.rm = T)) %>% 
  filter_all(all_vars(!is.infinite(.))) %>% 
  mutate(ice_off_doy_imputed = 
           round(if_else((froze %in% "Y" | froze %in% "U") & 
                           ice_off_doy %in% NA & sum(!is.na(ice_off_doy)) >= 20,
                         average_ice_off,if_else((froze %in% "N" & sum(!is.na(ice_off_doy)) >= 20),
                                                 earliest_thawing_date,
                                                 ice_off_doy))))%>% 
  select(LAKEID,year,ice_off_doy_imputed,PPT06:Tave05) 

#joining the hydrolakes data
ice_off_met_morph <- left_join(ice_off_ts_imputed,hydrolakes, by = "LAKEID") %>% 
  na.omit() %>% ungroup()
```

## Summery of the lakes

```{r}
#| warning: false
#| message: false
Lakes_analyzed <- inner_join(ice_on_met_morph %>% select(LAKEID),ice_off_met_morph %>% select(LAKEID)) %>% group_by(LAKEID) %>% summarise(count= n()) %>% left_join(hydrolakes, by =  "LAKEID")

```

### Data curation for models

Data for random forest regression

1.  Representative temperature and precipitation is calculated in two steps:

    1.  Twelve months (JFMAMJJASOND) temperature and precipitation are divided into 4 seasons (DJF i.e., month 12,01,02 - Winter : MAM i.e., month 03,04,05 - Spring : JJA i.e. month 06,07,08 - Summer : SON i.e., month 09,10,11 - Fall). Season means are calculated.

    2.  Ice-on dependent temperature is represented by sum of summer, fall and winter temperature while ice-off is represented as sum of winter and spring temperature. Ice-on dependent precipitation is represented by sum of winter and fall precipitation (Summer precipitation is considered not important) while ice-off precipitation is represented as sum of winter and fall temperatures.

2.  

    Data points for RF are retained in the following way:

    :   Without the exclusion inclusion criteria (i.e., full time series), with imputation method applied and removing NAs as explained in section [Ice records Imputation method].

```{r}
#| warning: false
#############
# functions #
#############

#Mean and rounding upto 2 decimal
fn_mean <- function(x1,x2,...){
  val <- mean(c(x1,x2,...), na.rm =T)
  val <- round(val,2)
  return(val)}
#Sum and rounding upto 2 decimal
fn_sum <- function(x1,x2,...){
  val <- sum(c(x1,x2,...), na.rm =T)
  val <- round(val,2)
  return(val)}


#Ice_on 
ice_on_rf <- ice_on_met_morph %>% 
  mutate(temp_summer = pmap_dbl(list(Tave06,Tave07,Tave08), fn_mean), 
         temp_fall = pmap_dbl(list(Tave09,Tave10,Tave11), fn_mean), 
         temp_winter = pmap_dbl(list(Tave12,Tave01,Tave02), fn_mean)) %>% 
  mutate(ppt_fall = pmap_dbl(list(PPT09,PPT10,PPT11), fn_mean), 
         ppt_winter = pmap_dbl(list(PPT12,PPT01,PPT02), fn_mean)) %>% 
  mutate(ice_on_temp = pmap_dbl(list(temp_summer,temp_fall,temp_winter),fn_sum)) %>% 
  mutate(ice_on_ppt = pmap_dbl(list(ppt_fall,ppt_winter),fn_sum)) %>%
  select(lat,lon,year,ice_on_doy_imputed,ice_on_temp,ice_on_ppt,Lake_area:Dmax_use_m) %>% 
  na.omit()

#Ice_off
ice_off_rf <- ice_off_met_morph %>% 
  mutate(temp_winter = pmap_dbl(list(Tave12,Tave01,Tave02), fn_mean), 
         temp_spring = pmap_dbl(list(Tave03,Tave04,Tave05), fn_mean)) %>% 
  mutate(ppt_winter = pmap_dbl(list(PPT12,PPT01,PPT02), fn_mean), 
         ppt_spring = pmap_dbl(list(PPT03,PPT04,PPT05), fn_mean)) %>% 
  mutate(ice_off_temp = pmap_dbl(list(temp_winter,temp_spring), fn_sum)) %>% 
  mutate(ice_off_ppt = pmap_dbl(list(ppt_winter,ppt_spring), fn_sum)) %>% 
  select(lat,lon,year,ice_off_doy_imputed,ice_off_temp,ice_off_ppt,Lake_area:Dmax_use_m) %>% 
  na.omit()
```

### Saving files for initial model comparison

```{r}
#| warning: false

fwrite(ice_on_rf, here::here("Data","ice_on_data_for_compairing_models.csv"))
fwrite(ice_off_rf, here::here("Data","ice_off_data_for_compairing_models.csv"))
```

Refer to the "`NH_lakes_ice_phenology_tested_models.qmd`" file for model comparison section.

## Historical trends of NH lake data

To analyze the historical trends of ice-on, ice-off and duration the data produced from the section [Data curation for historical trend] are used.

Since there is very little data available for year 2021 that year is dropped from analysis. The data is skewed for both ice-on and ice-off, for trend analysis some a point between ice-on 25% quantile (1974) and ice-off 25% quantile (1968) is taken for observing change in trend. Hence data from 1971 (past 50 years) is considered for analysis.

### Anomaly Calculation

In this case study anomalies for ice-on and ice-off is calculated using following method:

1.  50 year (1971-2020) average ice freezing and thawing was calculated using all lakes
2.  Anomalies are calculated by observing deviation from freezing or thawing dates from 50 Year mean

```{r}

# ice on anomalies
ice_on_anomaly <- NH_lake_ice_on_historical %>%
  filter(year >= 1971 & year <= 2020) %>% 
  mutate(ice_on_average = mean(ice_on_doy, na.rm =T)) %>% rowwise() %>% 
  mutate(ice_on_anomaly = ifelse(ice_on_doy %in% NA,NA,ice_on_doy - ice_on_average)) %>% 
  na.omit() 

# ice off anomalies
ice_off_anomaly <- NH_lake_ice_off_historical %>%
  filter(year >= 1971 & year <= 2020) %>% 
  mutate(ice_off_average = mean(ice_off_doy, na.rm =T)) %>% rowwise() %>% 
  mutate(ice_off_anomaly = ifelse(ice_off_doy %in% NA,NA,ice_off_average - ice_off_doy)) %>% 
  # Note: here avarage - ice-off computed to have positive as warming and negetive as cooling  
  na.omit() 
```

### Break-point analysis of ice-on and ice-off anomalies

Ice-on breakpoints : Use of AR(1) regression model to create residuals, which are then used for structural break tests. We used Quandt Likelihood Ratio (QLR) statistic using Fstats() command from the strucchange package.

```{r}
#| warning: false
breakpoint_dat_ice_on <- ice_on_anomaly %>% group_by(year) %>% 
  dplyr::summarise(ice_on_mean_anomaly = mean(ice_on_anomaly, na.rm =T)) %>% 
    select(ice_on_mean_anomaly)

#  TS values are placed in the first column, and a lag of the first column in the second.

breakpoint_dat_ice_on <- breakpoint_dat_ice_on %>% mutate(ylag0 = ice_on_mean_anomaly,
              ylag1 = lag(ice_on_mean_anomaly)
              ) %>%
  drop_na() %>% select(ylag0,ylag1)


ice_on_brk_qlr <- Fstats(ylag0 ~ ylag1 ,data = breakpoint_dat_ice_on)


bp_ice_on <- breakpoints(ice_on_brk_qlr, h = 0.15) #h = 0.15 means that each segment must have 15% of data of the TS to be used as segment for btrakpoint
bp_ice_on
# one breakpoint

ice_on_anomaly %>% group_by(year) %>% 
  dplyr::summarise(ice_on_mean_anomaly = mean(ice_on_anomaly, na.rm =T)) %>% 
    select(year,ice_on_mean_anomaly) %>% mutate(ylag0 = ice_on_mean_anomaly,
              ylag1 = lag(ice_on_mean_anomaly)
              ) %>%
  drop_na() %>% slice(ice_on_brk_qlr$breakpoint)
# breakpoint at 1989


sctest(ice_on_brk_qlr, type = "supF") #Chaw test
plot(ice_on_brk_qlr, alpha = 0.05)
# breakpoint is significant 



# plotting brekpoint
breakpoint_dat_ice_on <- ts(ice_on_anomaly %>% group_by(year) %>% 
  dplyr::summarise(ice_on_mean_anomaly = mean(ice_on_anomaly, na.rm =T)) %>% 
    select(ice_on_mean_anomaly), start = 1971, end = 2020, frequency = 1)

bp_ice_on <- breakpoints(breakpoint_dat_ice_on ~ 1, breaks = 1)
bp_ice_on

ci_ice_off <- confint(bp_ice_on, breaks = 1)

ci_ice_on <- confint(bp_ice_on, breaks = 1)
fm0 <- lm(breakpoint_dat_ice_on ~ 1)
bp_ice_on_fac <- breakfactor(bp_ice_on, breaks = 1)
fm1 <- lm(breakpoint_dat_ice_on ~ bp_ice_on_fac - 1)


# coef(fm0) # this value is the mean of time series 
# coef(fm1) # This values are the segment means

plot(breakpoint_dat_ice_on)+lines(ci_ice_on)+
  lines(ts(fitted(fm0), start = 1971), col = 3)+
  lines(ts(fitted(fm1), start = 1971), col = 4)+
  lines(bp_ice_on)
```

Ice-on break-point is at 1989

Ice-off break-point :

```{r}
#| warning: false

breakpoint_dat_ice_off <- ice_off_anomaly %>% group_by(year) %>% 
  dplyr::summarise(ice_off_mean_anomaly = mean(ice_off_anomaly, na.rm =T)) %>% 
    select(ice_off_mean_anomaly)

#  TS values are placed in the first column, and a lag of the first column in the second.

breakpoint_dat_ice_off <- breakpoint_dat_ice_off %>% mutate(ylag0 = ice_off_mean_anomaly,
              ylag1 = lag(ice_off_mean_anomaly)
              ) %>%
  drop_na() %>% select(ylag0,ylag1)

ice_off_brk_qlr <- Fstats(ylag0 ~ ylag1, data = breakpoint_dat_ice_off)
bp_ice_off <- breakpoints(ice_off_brk_qlr,h = 0.15) #h = 0.15 means that each segment must have 15% of data of the TS to be used as segment for btrakpoint
bp_ice_off
# one breakpoint
ice_off_anomaly %>% group_by(year) %>% 
  dplyr::summarise(ice_off_mean_anomaly = mean(ice_off_anomaly, na.rm =T)) %>% 
    select(year,ice_off_mean_anomaly) %>% mutate(ylag0 = ice_off_mean_anomaly,
              ylag1 = lag(ice_off_mean_anomaly)
              ) %>%
  drop_na() %>% slice(ice_off_brk_qlr$breakpoint)
# breakpoint at 1988

sctest(ice_off_brk_qlr, type = "supF") #Chaw test
plot(ice_off_brk_qlr, alpha = 0.05)
# breakpoint is significant 


# Ploting breakpoit
breakpoint_dat_ice_off <- ts(ice_off_anomaly %>% group_by(year) %>% 
  dplyr::summarise(ice_off_mean_anomaly = mean(ice_off_anomaly, na.rm =T)) %>% 
    select(ice_off_mean_anomaly), start = 1971, end = 2020, frequency = 1)

bp_ice_off <- breakpoints(breakpoint_dat_ice_off ~ 1, breaks = 1)
ci_ice_off <- confint(bp_ice_off, breaks = 1)

fm0 <- lm(breakpoint_dat_ice_off ~ 1)
bp_ice_off_fac <- breakfactor(bp_ice_off, breaks = 1)
fm1 <- lm(breakpoint_dat_ice_off ~ bp_ice_off_fac - 1)

# coef(fm0) # this value is the mean of time series 
# coef(fm1) # This values are the segment means

plot(breakpoint_dat_ice_off, cex.axis=3)+lines(ci_ice_off)+
  lines(ts(fitted(fm0), start = 1971), col = 3)+
  lines(ts(fitted(fm1), start = 1971), col = 4)+
  lines(bp_ice_off)

```

Ice-off break-point is at 1988

### Ice-cover duration and Sen's slope analysis

```{r}
#| warning: false

# Average ice duration
mk_sens <- ice_duration_hist  %>% filter(year >= 1971 & year <= 2020) %>% group_by(year) %>% 
  summarise(duration_avrg = mean(duration, na.rm =T))

# Mann-Kendall test
mk.test(mk_sens$duration_avrg)

# Sen's slope 
sens.slope(mk_sens$duration_avrg)
# Coeeficients of Sen's slope 
coef(mblm(duration_avrg~year,mk_sens,repeated = F))

ggplot(dat2, aes(x = year, y = duration_avrg)) +
  ggtitle("Ice Duration Trend")  +
  geom_label(aes( x=1980, y=110, label="Sen's slope 
 -0.903 "), size = 7)+
  geom_abline(slope = -0.9028964,intercept =1943.3115626, color = "darkblue", size = 1)+
  geom_point(shape = 21, fill = "#808080",  color = "black" ,size = 3)+
  labs(x = "Years", y = "Duration of ice cover on lakes (Days)")+
  theme(axis.text = element_text(size = rel(1.5)),
        axis.title.y = element_text(size = rel(2)),
        axis.title.x = element_text(size = rel(2)),
        legend.title = element_text( size = 17,vjust = .5),
        legend.text = element_text(size = 14),
        plot.title = element_text(size=22),
        panel.background = element_rect(fill = "#EEEEEE",
                                        colour = "#EEEEEE",
                                        size = 0.5, linetype = "solid"),
        panel.grid.major = element_line(size = 0.5, linetype = 'solid',
                                        colour = "white"), 
        panel.grid.minor = element_line(size = 0.25, linetype = 'solid',
                                        colour = "white")
  )

```

```{r}

breakpoint_dat_dur <- ice_duration_hist %>% filter(year >= 1971 & year <= 2020) %>% group_by(year) %>% 
  dplyr::summarise(mean_duration = mean(duration, na.rm =T)) %>% 
    select(mean_duration)

#  TS values are placed in the first column, and a lag of the first column in the second.

breakpoint_dat_dur <- breakpoint_dat_dur %>% mutate(ylag0 = mean_duration,
              ylag1 = lag(mean_duration)
              ) %>%
  drop_na() %>% select(ylag0,ylag1)

ice_off_brk_qlr <- Fstats(ylag0 ~ ylag1, data = breakpoint_dat_dur)
bp_ice_off <- breakpoints(ice_off_brk_qlr,h = 0.15) #h = 0.15 means that each segment must have 15% of data of the TS to be used as segment for btrakpoint
bp_ice_off


# one breakpoint
ice_duration_hist %>% filter(year >= 1971 & year <= 2020) %>% group_by(year) %>% 
  dplyr::summarise(mean_duration = mean(duration, na.rm =T)) %>% 
    select(year,mean_duration) %>% mutate(ylag0 = mean_duration,
              ylag1 = lag(mean_duration)
              ) %>%
  drop_na() %>% slice(ice_off_brk_qlr$breakpoint)
# breakpoint at 1988

sctest(ice_off_brk_qlr, type = "supF") #Chaw test
plot(ice_off_brk_qlr, alpha = 0.05)
# breakpoint is significant 


# Ploting breakpoit
breakpoint_dat_dur <- ts(ice_duration_hist %>% filter(year >= 1971 & year <= 2020) %>% group_by(year) %>% 
  dplyr::summarise(mean_duration = mean(duration, na.rm =T)) %>% 
    select(mean_duration), start = 1971, end = 2020, frequency = 1)

bp_ice_dur <- breakpoints(breakpoint_dat_dur ~ 1, breaks = 1)
ci_ice_dur <- confint(bp_ice_dur, breaks = 1)

fm0 <- lm(breakpoint_dat_dur ~ 1)
bp_ice_dur_fac <- breakfactor(bp_ice_dur, breaks = 1)
fm1 <- lm(breakpoint_dat_dur ~ bp_ice_dur_fac - 1)

# coef(fm0) # this value is the mean of time series 
# coef(fm1) # This values are the segment means

plot(breakpoint_dat_dur, cex.axis=3)+lines(ci_ice_dur)+
  lines(ts(fitted(fm0), start = 1971), col = 3)+
  lines(ts(fitted(fm1), start = 1971), col = 4)+
  lines(bp_ice_dur)

```

## Random forest models

After compairing multiple machine learning model performances (here is the link to 1. the model files and 2. model script) we used the best performing model, the Random Forest to continue further analysis. We deployed spatiotemporal splits for further building of models used for predictions.

### Spatiotemporal Split

Ice-on

```{r}
#sf object for ice_on points
ice_on_loations <- sf::st_as_sf(ice_on_rf %>% select(lon,lat), coords = c("lon", "lat"), crs = 4326)

ice_on_spatial_segments <- cv_spatial(x = ice_on_loations,
                                      size = 250000, # size of the blocks in metres
                                      k = 10, # number of folds
                                      hexagon = TRUE, # use hexagonal blocks
                                      selection = "random", # random blocks-to-fold
                                      iteration = 100, # to find evenly dispersed folds
                                      biomod2 = F,
                                      seed = 20086)


# Attching the spatial folds
ice_on_lakes_spatial <-  ice_on_rf %>% 
  mutate(space_id = ice_on_spatial_segments[["folds_ids"]])%>% select(lat:Dmax_use_m,space_id)

#Randomizing rows
set.seed(7834)  
ice_on_lakes_spatial_random_row <- ice_on_lakes_spatial[sample(nrow(ice_on_lakes_spatial)), ]

# Dividing the timeline into 10 equal time-folds
ice_on_lakes_spatiotemporal <- ice_on_lakes_spatial_random_row %>% 
  arrange(year) %>% mutate(time_quantile = ntile(year, 10)) %>% select(-year)


# Spacetime group test split with randomly taking 20% of data from each combination
ice_on_lakes_spatiotemporal_test <- ice_on_lakes_spatiotemporal %>%
  group_by(space_id,time_quantile) %>%
  slice_sample(prop = 0.20) %>%
  ungroup() 

# rest of the data (Training set)
ice_on_lakes_spatiotemporal_train <- ice_on_lakes_spatiotemporal %>%
  anti_join(ice_on_lakes_spatiotemporal_test, by = NULL)

```

Ice-off

```{r}
#sf object for ice_off points
ice_off_loations <- sf::st_as_sf(ice_off_rf %>% select(lon,lat), coords = c("lon", "lat"), crs = 4326)

ice_off_spatial_segments <- cv_spatial(x = ice_off_loations,
                                      size = 250000, # size of the blocks in metres
                                      k = 10, # number of folds
                                      hexagon = TRUE, # use hexagonal blocks
                                      selection = "random", # random blocks-to-fold
                                      iteration = 100, # to find evenly dispersed folds
                                      biomod2 = F,
                                      seed = 20345)


# Attching the spatial folds
ice_off_lakes_spatial <-  ice_off_rf %>% 
  mutate(space_id = ice_off_spatial_segments[["folds_ids"]])%>% select(lat:Dmax_use_m,space_id)

#Randomizing rows
set.seed(7834)  
ice_off_lakes_spatial_random_row <- ice_off_lakes_spatial[sample(nrow(ice_off_lakes_spatial)), ]

# Dividing the timeline into 10 equal time-folds
ice_off_lakes_spatiotemporal <- ice_off_lakes_spatial_random_row %>% 
  arrange(year) %>% mutate(time_quantile = ntile(year, 10)) %>% select(-year)


# Spacetime group test split with randomly taking 20% of data from each combination
ice_off_lakes_spatiotemporal_test <- ice_off_lakes_spatiotemporal %>%
  group_by(space_id,time_quantile) %>%
  slice_sample(prop = 0.20) %>%
  ungroup() 

# rest of the data (Training set)
ice_off_lakes_spatiotemporal_train <- ice_off_lakes_spatiotemporal %>%
  anti_join(ice_off_lakes_spatiotemporal_test, by = NULL)

```

### Saving files

files will be saved for two distinct purposes

1.  Visualizing and analyzing historical freezing and thawing patterns (without removing NAs)
2.  Version_1 and Version_2 of RF data

```{r}

#Visualizing and analyzing historical freezing and thawing patterns
#Ice-on
fwrite(NH_lake_ice_on_historical %>% select(LAKEID,year,froze,ice_on_doy),
       here::here("Data","NH_lakes_ice_on_historical.csv"))
#Ice-off
fwrite(NH_lake_ice_off_historical %>% select(LAKEID,year,froze,ice_off_doy),
       here::here("Data","NH_lakes_ice_off_historical.csv"))
#Ice-cover duration
fwrite(ice_duration_hist, 
       here::here("Data","NH_lakes_ice_duration_historical.csv"))

# Ice-on 
fwrite(ice_on_lakes_spatiotemporal_train, here::here("Data","NH_lakes_ice_on_train.csv"))
fwrite(ice_on_lakes_spatiotemporal_test, here::here("Data","NH_lakes_ice_on_test.csv"))

#Ice-off
fwrite(ice_off_lakes_spatiotemporal_train,here::here("Data","NH_lakes_ice_off_train.csv"))
fwrite(ice_off_lakes_spatiotemporal_test,here::here("Data","NH_lakes_ice_off_test.csv"))

```

## Random Forest Regression Model

### Ice-on Model

```{r}
#| warning: false
#| eval: false

future::plan("multisession")

# Define task and learner
tsk_ice_on = TaskRegrST$new(ice_on_lakes_spatiotemporal_train, id = "ice_on_task",
                            target = "ice_on_doy_imputed",coords_as_features = FALSE,
                            coordinate_names = c("lon", "lat"), crs = 4326)

tsk_ice_on$set_col_roles("time_quantile", roles = "time")
tsk_ice_on$set_col_roles("space_id", roles = "space")


lrn_rf <- lrn("regr.ranger",mtry  = to_tune(2:5),
              num.trees = to_tune(100,600))


tnr_grid_search = tnr("grid_search", resolution = 5, batch_size = 10)

# re-sampling strategy
rsmp_cv5 <- rsmp("sptcv_cstf", folds = 10)


ice_on_rf_at_model = auto_tuner(
  tuner = tnr_grid_search,
  learner = lrn_rf,
  resampling = rsmp_cv5
)

ice_on_pseudo_split = mlr3::partition(tsk_ice_on, ratio = 1)

set.seed(2723)
ice_on_rf_at_model$train(tsk_ice_on, row_ids = ice_on_pseudo_split$train)

write_rds(ice_on_rf_at_model,"ice_on_rf_at_model.rds")


```

### Model Evaluation

```{r}
ice_on_rf_at_model <- read_rds(here::here("Data","Models","ice_on_rf_at_model.rds"))
vals <- ice_on_rf_at_model$predict_newdata(ice_on_lakes_spatiotemporal_test)
out <- data.frame(cbind(vals$response,vals$truth))

out <- out %>% rename(response = X1,truth = X2)

###### modified from: https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r #########

eval_results <- function(true, predicted) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/length(true))
  MAE = sum(abs(true - predicted))/length(true)
 
  
  # Model performance metrics
  tibble(RSquare = R_square,
             RMSE = RMSE,
             MAE = MAE)
}

#######

eval_results(out$truth,out$response)

r_sq <-eval_results(out$truth,out$response)[,1]
rmse_ice_on <- eval_results(out$truth,out$response)[,2]
mae_ice_on <- eval_results(out$truth,out$response)[,3]

out %>%
  ggplot(aes(response, truth))+
  geom_point(aes(fill = "Data"))+
  geom_smooth(method = "lm", se=FALSE, aes(color = "Fit"))+
  stat_regline_equation(label.x= -110, label.y=150-70, color = "Maroon")+
  stat_cor(aes(label=..rr.label..), label.x=-110, label.y=140-70, color = "Maroon")+
  geom_abline(intercept = 0, col = "Blue", size = .8)+
  annotate(geom="text", x= -100, y=172-70, label= print(paste0("MAE = ", round(mae_ice_on, 2))),
           color="Maroon")+
  annotate(geom="text", x= -100, y=161-70, label=print(paste0("RMSE = ", round(rmse_ice_on, 2))),
           color="Maroon")+
  labs(x= "Predicted Dates",y= "Observed Dates", fill = "", col="", title = "Ice-on model")+
  theme(
    axis.text = element_text(size = rel(1.5)),
    axis.title.y = element_text(size = rel(2)),
    axis.title.x = element_text(size = rel(2)),
    legend.title = element_text( size = 17,vjust = .5),
    legend.text = element_text(size = 14),
    plot.title = element_text(size=22))+
  scale_color_manual( values = c("Fit" = "red",
                                 "X = Y" = "blue"))
```

### Ice-off Model

```{r}

future::plan("multisession")

# Define task and learner
tsk_ice_off = TaskRegrST$new(ice_off_lakes_spatiotemporal_train, id = "ice_off_task",
                            target = "ice_off_doy_imputed",coords_as_features = FALSE,
                            coordinate_names = c("lon", "lat"), crs = 4326)

tsk_ice_off$set_col_roles("time_quantile", roles = "time")
tsk_ice_off$set_col_roles("space_id", roles = "space")


lrn_rf <- lrn("regr.ranger",mtry  = to_tune(2:5),
              num.trees = to_tune(100,600))


tnr_grid_search = tnr("grid_search", resolution = 5, batch_size = 10)

# re-sampling strategy
rsmp_cv5 <- rsmp("sptcv_cstf", folds = 10)


ice_off_rf_at_model = auto_tuner(
  tuner = tnr_grid_search,
  learner = lrn_rf,
  resampling = rsmp_cv5)

```

```{r}
#| warning: false
#| eval: false

ice_off_pseudo_split = mlr3::partition(tsk_ice_off, ratio = 1)

set.seed(2723)
ice_off_rf_at_model$train(tsk_ice_off, row_ids = ice_off_pseudo_split$train)

write_rds(ice_off_rf_at_model,"ice_off_rf_at_model.rds")


```

### Model Evaluation

```{r}

ice_off_rf_at_model <- read_rds(here::here("Data","Models","ice_off_rf_at_model.rds"))
vals <- ice_off_rf_at_model$predict_newdata(ice_off_lakes_spatiotemporal_test)
out <- data.frame(cbind(vals$response,vals$truth))

out <- out %>% rename(response = X1,truth = X2)
###### modified from: https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r #########

eval_results <- function(true, predicted) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/length(true))
  MAE = sum(abs(true - predicted))/length(true)
 
  
  # Model performance metrics
  tibble(RSquare = R_square,
             RMSE = RMSE,
             MAE = MAE)
}

#######

eval_results(out$truth,out$response)

r_sq <-eval_results(out$truth,out$response)[,1]
rmse_ice_off <- eval_results(out$truth,out$response)[,2]
mae_ice_off <- eval_results(out$truth,out$response)[,3]

out %>%
  ggplot(aes(response, truth))+
  geom_point(aes(fill = "Data"))+
  geom_smooth(method = "lm", se=FALSE, aes(color = "Fit"))+
  stat_regline_equation(label.x= 25, label.y=147+40, color = "Maroon")+
  stat_cor(aes(label=..rr.label..), label.x=25, label.y=140+40, color = "Maroon")+
  geom_abline(intercept = 0, col = "Blue", size = .8)+
  annotate(geom="text", x= 25+20, y=165+40, label= print(paste0("MAE = ", round(mae_ice_off, 2))),
           color="Maroon")+
  annotate(geom="text", x= 25+21, y=157+40, label=print(paste0("RMSE = ", round(rmse_ice_off, 2))),
           color="Maroon")+
  labs(x= "Predicted Dates",y= "Observed Dates", fill = "", col="", title = "Ice-off model")+
  theme(
    axis.text = element_text(size = rel(1.5)),
    axis.title.y = element_text(size = rel(2)),
    axis.title.x = element_text(size = rel(2)),
    legend.title = element_text( size = 17,vjust = .5),
    legend.text = element_text(size = 14),
    plot.title = element_text(size=22))+
  scale_color_manual( values = c("Fit" = "red",
                                 "X = Y" = "blue"))
```

## Random Forest Classification Model

```{r}
#| warning: false
#| eval: false

# see section <reading files> for NH_lakes_ts and hydrolakes files
NH_lakes_met_morph <- NH_lakes_ts %>% left_join(hydrolakes, by = "LAKEID") 


#Mean and rounding upto 2 decimal
fn_mean <- function(x1,x2,...){
  val <- mean(c(x1,x2,...), na.rm =T)
  val <- round(val,2)
  return(val)}
#Sum and rounding upto 2 decimal
fn_sum <- function(x1,x2,...){
  val <- sum(c(x1,x2,...), na.rm =T)
  val <- round(val,2)
  return(val)}


#Ice_on 
ice_froze <- NH_lakes_met_morph %>% 
  select(lat,lon,year,froze,PPT06:Tave05,Lake_area:Dmax_use_m) %>%
  na.omit() %>% filter(froze %in% c("Y","N")) %>% 
  mutate(temp_summer = pmap_dbl(list(Tave06,Tave07,Tave08), fn_mean), 
         temp_fall = pmap_dbl(list(Tave09,Tave10,Tave11), fn_mean), 
         temp_winter = pmap_dbl(list(Tave12,Tave01,Tave02), fn_mean)) %>% 
  mutate(ppt_fall = pmap_dbl(list(PPT09,PPT10,PPT11), fn_mean), 
         ppt_winter = pmap_dbl(list(PPT12,PPT01,PPT02), fn_mean)) %>% 
  mutate(ice_on_temp = pmap_dbl(list(temp_summer,temp_fall,temp_winter),fn_sum)) %>% 
  mutate(ice_on_ppt = pmap_dbl(list(ppt_fall,ppt_winter),fn_sum)) %>%
  mutate(temp_winter = pmap_dbl(list(Tave12,Tave01,Tave02), fn_mean), 
         temp_spring = pmap_dbl(list(Tave03,Tave04,Tave05), fn_mean)) %>% 
  mutate(ppt_winter = pmap_dbl(list(PPT12,PPT01,PPT02), fn_mean), 
         ppt_spring = pmap_dbl(list(PPT03,PPT04,PPT05), fn_mean)) %>% 
  mutate(ice_off_temp = pmap_dbl(list(temp_winter,temp_spring), fn_sum)) %>% 
  mutate(ice_off_ppt = pmap_dbl(list(ppt_winter,ppt_spring), fn_sum)) %>% 
  select(lat,lon,year,froze,ice_on_temp,ice_on_ppt,ice_off_temp,ice_off_ppt,Lake_area:Dmax_use_m)


#sf object for ice_on points
ice_froze_loations <- sf::st_as_sf(ice_froze %>% select(lon,lat), coords = c("lon", "lat"), crs = 4326)

ice_froze_spatial_segments <- cv_spatial(x = ice_froze_loations,
                                      size = 250000, # size of the blocks in metres
                                      k = 10, # number of folds
                                      hexagon = TRUE, # use hexagonal blocks
                                      selection = "random", # random blocks-to-fold
                                      iteration = 100, # to find evenly dispersed folds
                                      biomod2 = F,
                                      seed = 20345)


# Attching the spatial folds
ice_froze_lakes_spatial <-  ice_froze  %>% 
  mutate(space_id = ice_froze_spatial_segments[["folds_ids"]])%>% select(lat:Dmax_use_m,space_id)

#Randomizing rows
set.seed(7834)  
ice_froze_lakes_spatial_random_row <- ice_froze_lakes_spatial[sample(nrow(ice_froze_lakes_spatial)), ]

# Dividing the timeline into 10 equal time-folds
ice_froze_lakes_spatiotemporal <- ice_froze_lakes_spatial_random_row %>% 
  arrange(year) %>% mutate(time_quantile = ntile(year, 10)) %>% select(-year)


# Spacetime group test split with randomly taking 20% of data from each combinatifroze
ice_froze_lakes_spatiotemporal_test <- ice_froze_lakes_spatiotemporal %>%
  group_by(space_id,time_quantile) %>%
  slice_sample(prop = 0.20) %>%
  ungroup() %>% mutate(froze = as.factor(froze))

# rest of the data (Training set)
ice_froze_lakes_spatiotemporal_train <- ice_froze_lakes_spatiotemporal %>%
  anti_join(ice_froze_lakes_spatiotemporal_test, by = NULL) %>% 
  mutate(froze = as.factor(froze))
```

### Froze Model

```{r}
#| warning: false
#| eval: false

future::plan("multisession")

# Define task and learner
tsk_ice_froze = TaskClassifST$new(ice_froze_lakes_spatiotemporal_train, id = "ice_froze_task",
                            target = "froze",coords_as_features = FALSE,
                            coordinate_names = c("lon", "lat"), crs = 4326)

tsk_ice_froze$set_col_roles("time_quantile", roles = "time")
tsk_ice_froze$set_col_roles("space_id", roles = "space")


lrn_rf <- lrn("classif.ranger",mtry  = to_tune(2:5),
              num.trees = to_tune(100,600), predict_type = "prob")


tnr_grid_search = tnr("grid_search", resolution = 5, batch_size = 10)

# re-sampling strategy
rsmp_cv5 <- rsmp("sptcv_cstf", folds = 10)


ice_froze_rf_at_model = auto_tuner(
  tuner = tnr_grid_search,
  learner = lrn_rf,
  resampling = rsmp_cv5
)

ice_froze_pseudo_split = mlr3::partition(tsk_ice_froze, ratio = 1)

set.seed(2723)
ice_froze_rf_at_model$train(tsk_ice_froze, row_ids = ice_froze_pseudo_split$train)

write_rds(ice_froze_rf_at_model,"ice_froze_rf_at_model.rds")
```

### Model Evaluation

```{r}

ice_froze_rf_at_model <- read_rds(here::here("Data","Models","ice_froze_rf_at_model.rds"))
vals <- ice_froze_rf_at_model$predict_newdata(ice_froze_lakes_spatiotemporal_test)

mlr3measures::confusion_matrix(truth = vals$truth,
  response = vals$response, positive = tsk_ice_froze$positive)

autoplot(vals, type = "roc")
```
